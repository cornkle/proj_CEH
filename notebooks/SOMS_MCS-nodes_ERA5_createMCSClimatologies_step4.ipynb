{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as pylab\n",
    "import cartopy\n",
    "from utils import constants as cnst\n",
    "from eod import msg\n",
    "from utils import u_grid, u_interpolate as u_int, constants as cnst, u_arrays, u_darrays, u_met\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as feature\n",
    "import cartopy.io.shapereader as shapereader\n",
    "import pickle as pkl\n",
    "import glob\n",
    "\n",
    "coast = shapereader.natural_earth(resolution='110m',\n",
    "                                  category='physical',\n",
    "                                  name='coastline')\n",
    "\n",
    "countries = shapereader.natural_earth(resolution='110m',\n",
    "                                      category='cultural',\n",
    "                                      name='admin_0_countries')\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (50., 50.)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ck/pythonWorkspace/proj_CEH\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filetag = 'pl'\n",
    "era_hour = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node_days = pkl.load(open(cnst.network_data + 'data/SOMS/saves/node3_MCS_matchTable.p', \"rb\"))\n",
    "node_days = pd.read_csv(cnst.DATA + '/SOMS/saves/3x3_Nodes_fullYear/mcs_matchtable/node3_MCS_matchTable_mergedNodes_5mmMCS_fullYear.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2004-04-09 05:30:00')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(node_days.index)[0].replace(hour=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_node_clim_old(filetag, node, era_hour):\n",
    "    \n",
    "    nodestr = 'node'+str(node)\n",
    "    node_days = pd.read_csv(cnst.DATA + '/SOMS/saves/3x3_Nodes_fullYear/mcs_matchtable/'+nodestr+'_MCS_matchTable_mergedNodes_5mmMCS_fullYear.csv', index_col=0, parse_dates=True)\n",
    "    #node_days = pkl.load(open(cnst.network_data + 'data/SOMS/saves/3x3/MCStable_3x3_5-9N/'+nodestr+'_MCS_matchTable.p', \"rb\"))\n",
    "    \n",
    "    #### potentially reduce MCS box here\n",
    "    region = (node_days.lat>5) & (node_days.lat<8) & (node_days.lon>-10) & (node_days.lon<10)\n",
    "    node_days = node_days[region]\n",
    "    \n",
    "    node_days_era = []\n",
    "    for nd in node_days.index:\n",
    "        node_days_era.append(nd.replace(hour=era_hour))\n",
    "    node_days_era = pd.Series(node_days_era).values\n",
    "        \n",
    "    if filetag == 'pl':\n",
    "        fft = 'pressure_levels'\n",
    "    else:\n",
    "        fft = 'surface'\n",
    "        \n",
    "    filebase = cnst.other_drive + '/ERA5_WAf/hourly/' \n",
    "    filepath = filebase + fft+'/ERA5*'+filetag+'.nc'\n",
    "    out = cnst.DATA + 'SOMS/saves/3x3_Nodes_fullYear/'+nodestr+'_ERA5_5MCS_5mm_mean_5-8N_old_'+filetag+'.nc'\n",
    "    outdev = cnst.DATA + 'SOMS/saves/3x3_Nodes_fullYear/'+nodestr+'_ERA5_5MCS_5mm_stddev_5-8N_old_'+filetag+'.nc'\n",
    "    \n",
    "    def means(da, node_days):\n",
    "        da = da.sel(time=(da['time.hour']==era_hour))\n",
    "        try:\n",
    "            da = da.sel(level=[925,850,750,650,550,250])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if (filetag == 'pl'):\n",
    "            if not 'z' in da.data_vars:\n",
    "                ipdb.set_trace()\n",
    "                \n",
    "        dal = []\n",
    "        \n",
    "        u, inv = np.unique(node_days, return_inverse=True)\n",
    "        n = np.bincount(inv)\n",
    "        goodinds = u[(n>=5)] #minimum 5 MCS per MCS day\n",
    "        \n",
    "        intersect_dates = np.intersect1d(goodinds, da.time.values)\n",
    "        if len(intersect_dates) == 0:\n",
    "            return None\n",
    "        try:\n",
    "            da = da.sel(time=intersect_dates)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        da = u_darrays.flip_lat(da)\n",
    "        return da\n",
    "    \n",
    "    srfc_list = []\n",
    "    \n",
    "    for f in glob.glob(filepath):\n",
    "        print('Doing', f)\n",
    "        srfc = xr.open_dataset(f) \n",
    "        srfc_concat = means(srfc,node_days_era)\n",
    "        if srfc_concat is None:\n",
    "            continue\n",
    "\n",
    "        srfc_list.append(srfc_concat)\n",
    "    \n",
    "    full = xr.concat(srfc_list, dim='time')\n",
    "    #ipdb.set_trace()\n",
    "    mean = full.mean('time')\n",
    "    dev = full.std('time')\n",
    "    \n",
    "    mean.to_netcdf(out)\n",
    "    dev.to_netcdf(outdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_node_clim(filetag, node, era_hour):\n",
    "    \n",
    "    nodestr = 'node'+str(node)\n",
    "    node_days = pd.read_csv(cnst.DATA + '/SOMS/saves/3x3_Nodes_fullYear/mcs_matchtable/'+nodestr+'_MCS_matchTable_mergedNodes_5mmMCS_fullYear.csv', index_col=0, parse_dates=True)\n",
    "    #node_days = pkl.load(open(cnst.network_data + 'data/SOMS/saves/3x3/MCStable_3x3_5-9N/'+nodestr+'_MCS_matchTable.p', \"rb\"))\n",
    "    \n",
    "    #### potentially reduce MCS box here\n",
    "    region = (node_days.lat>5) & (node_days.lat<8) & (node_days.lon>-10) & (node_days.lon<10)\n",
    "    node_days = node_days[region]\n",
    "    \n",
    "#     node_days_era = []\n",
    "#     for nd in node_days.index:\n",
    "#         node_days_era.append(nd.replace(hour=era_hour))\n",
    "#     node_days_era = pd.Series(node_days_era).values\n",
    "\n",
    "    node_days_era = []\n",
    "    for nd in node_days.index:\n",
    "        node_days_era.append(nd)\n",
    "    node_days_era = pd.Series(node_days_era).values\n",
    "        \n",
    "    if filetag == 'pl':\n",
    "        fft = 'pressure_levels'\n",
    "    else:\n",
    "        fft = 'surface'\n",
    "        \n",
    "    filebase = cnst.other_drive + '/ERA5_WAf/hourly/' \n",
    "    filepath = filebase + fft+'/ERA5*'+filetag+'.nc'\n",
    "    out = cnst.DATA + 'SOMS/saves/3x3_Nodes_fullYear/'+nodestr+'_ERA5_5MCS_5mm_mean_5-8N_5MCS_'+filetag+'.nc'\n",
    "    outdev = cnst.DATA + 'SOMS/saves/3x3_Nodes_fullYear/'+nodestr+'_ERA5_5MCS_5mm_stddev_5-8N_5MCS_'+filetag+'.nc'\n",
    "    \n",
    "    def means(da, node_days):\n",
    "        da = da.sel(time=(da['time.hour']==era_hour))\n",
    "        try:\n",
    "            da = da.sel(level=[925,850,750,650,550,250])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if (filetag == 'pl'):\n",
    "            if not 'z' in da.data_vars:\n",
    "                ipdb.set_trace()\n",
    "                \n",
    "        dal = []\n",
    "\n",
    "        u, inv = np.unique(node_days, return_inverse=True)\n",
    "        n = np.bincount(inv)\n",
    "        goodinds = u[(n>=3)] #minimum 5 MCS per MCS day (5 in one hour)\n",
    "        goodinds = pd.Series(goodinds)\n",
    "        goodins_erahour = []\n",
    "   \n",
    "        for nd in goodinds:\n",
    "            goodins_erahour.append(nd.replace(hour=era_hour, minute=0))\n",
    "        \n",
    "        goodins_erahour = np.unique(goodins_erahour)\n",
    "        goodins_erahour = pd.Series(goodins_erahour).values\n",
    "        \n",
    "        #ipdb.set_trace()\n",
    "        intersect_dates = np.intersect1d(goodins_erahour, da.time.values)\n",
    "        \n",
    "        if len(intersect_dates) == 0:\n",
    "            return None\n",
    "        try:\n",
    "            da = da.sel(time=intersect_dates)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        da = u_darrays.flip_lat(da)\n",
    "        return da\n",
    "    \n",
    "    srfc_list = []\n",
    "    \n",
    "    for f in glob.glob(filepath):\n",
    "        #print('Doing', f)\n",
    "        srfc = xr.open_dataset(f) \n",
    "        srfc_concat = means(srfc,node_days_era)\n",
    "        if srfc_concat is None:\n",
    "            continue\n",
    "\n",
    "        srfc_list.append(srfc_concat)\n",
    "    \n",
    "    print(node, 'saved MCS days nb', len(srfc_list), len(node_days), len(srfc_list)/len(node_days)*100)\n",
    "    full = xr.concat(srfc_list, dim='time')\n",
    "    #ipdb.set_trace()\n",
    "    mean = full.mean('time')\n",
    "    dev = full.std('time')\n",
    "    \n",
    "#     mean.to_netcdf(out)\n",
    "#     dev.to_netcdf(outdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 saved MCS days nb 61 2225 2.741573033707865\n",
      "2 saved MCS days nb 65 2322 2.7993109388458226\n",
      "3 saved MCS days nb 56 1751 3.198172472872644\n",
      "4 saved MCS days nb 56 1270 4.409448818897638\n",
      "5 saved MCS days nb 59 1865 3.1635388739946384\n",
      "6 saved MCS days nb 56 1763 3.176403857061826\n",
      "7 saved MCS days nb 23 647 3.554868624420402\n",
      "8 saved MCS days nb 41 1291 3.1758326878388843\n",
      "9 saved MCS days nb 54 1866 2.8938906752411575\n"
     ]
    }
   ],
   "source": [
    "for ni in range(1,10):\n",
    "#     nodestr='node'+str(1)\n",
    "#     all_node_file = pkl.load(open(cnst.network_data + 'data/SOMS/saves/node_days.p', \"rb\"))\n",
    "#     node_days = all_node_file[nodestr]\n",
    "#     node_days_era = []\n",
    "#     for nd in node_days:\n",
    "#         node_days_era.append(nd.replace(hour=era_hour))\n",
    "        \n",
    "    save_node_clim(filetag, ni, era_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(9,5), dpi=200)\n",
    "\n",
    "toconcat = []\n",
    "for node in range(1,10):\n",
    "    toconcat.append(pd.read_csv(cnst.DATA + '/SOMS/saves/3x3_Nodes_fullYear/mcs_matchtable/node'+str(node)+'_MCS_matchTable_mergedNodes_5mmMCS_fullYear.csv', index_col=0, parse_dates=True))\n",
    "    \n",
    "df = pd.concat(toconcat)\n",
    "df = df.where(df.hour==18).dropna()\n",
    "    \n",
    "nmonth = []\n",
    "ax = f.add_subplot(1,1,1)\n",
    "\n",
    "mdays = [31,28,31,30,31,30,31,31,30,31,30,31]\n",
    "for ni in range(1,13):\n",
    "    nmonth.append(np.sum(df.month==ni)/(mdays[ni-1]*np.unique(df.year).size)) # note, divided by number of days in years 2004-2015! \n",
    "    print(np.unique(df.year))\n",
    "print(nmonth)\n",
    "ax.bar(np.arange(1,13).astype(int), np.array(nmonth))\n",
    "ax.set_ylabel('MCS per day [considering MCS days only]')\n",
    "ax.set_xlabel('Months')\n",
    "#ax.set_title('Node:'+ str(node)+' '+merged_nodes_str[node-1])\n",
    "ax.set_xticks(np.arange(1,13).astype(int))\n",
    "    \n",
    "plt.tight_layout()\n",
    "#f.savefig(cnst.network_data + 'figs/SOMS/nb_node_permonth_nodes_fullYear.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_node_file = pkl.load(open(cnst.network_data + 'data/SOMS/saves/3x3_Nodes_fullYear/node_days.p', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_node_file['node1']).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import u_darrays as uda\n",
    "f = plt.figure(figsize=(15,5), dpi=300)\n",
    "mname = 'node'\n",
    "\n",
    "for ids, m in enumerate(range(1,10)):\n",
    "    \n",
    "    \n",
    "    pl_clim = xr.open_dataset(cnst.DATA + 'SOMS/saves/3x3_Nodes_fullYear/node1_ERA5_clim_mean_srfc.nc')\n",
    "    #pl_clim = uda.flip_lat(pl_clim)\n",
    "    pl_clim = pl_clim['cape'].sel(latitude=slice(3,10), longitude=slice(-12,12))\n",
    "    pl_clim.values = pl_clim.values*0\n",
    "    \n",
    "    df = pd.read_csv(cnst.DATA + '/SOMS/saves/3x3_Nodes_fullYear/mcs_matchtable/node'+str(m)+'_MCS_matchTable_mergedNodes_5mmMCS_fullYear.csv', index_col=0, parse_dates=True)\n",
    "    df = df.where(df.hour==18).dropna()\n",
    "    anf = all_node_file['node'+str(m)]\n",
    "    #ipdb.set_trace()\n",
    "    anf = anf.where((anf.year>=2004)&(anf.year<=2015)).dropna()\n",
    "    \n",
    "    daycount = df.index.size/anf.size #*100 \n",
    "    #ipdb.set_trace()\n",
    "\n",
    "    for ix, dff in df.iterrows():\n",
    "\n",
    "        xv = dff.lon\n",
    "        yv = dff.lat\n",
    "\n",
    "        loc = pl_clim.sel(longitude=xv, latitude=yv, method='nearest')\n",
    "        xm, ym = np.meshgrid(pl_clim.longitude.values, pl_clim.latitude.values)\n",
    "        ipos = np.where((xm==loc.longitude.values)&(ym==loc.latitude.values))\n",
    "        try:\n",
    "            pl_clim.values[ipos] = pl_clim.values[ipos]+1\n",
    "        except:\n",
    "            ipdb.set_trace()\n",
    "\n",
    "    plots = pl_clim.values #/ anf.size *100\n",
    "    \n",
    "    ax = f.add_subplot(3,3,ids+1, projection=ccrs.PlateCarree())\n",
    "\n",
    "    plt.contourf(pl_clim.longitude, pl_clim.latitude, plots, transform=ccrs.PlateCarree(), cmap='PuBuGn', extend='both', levels=np.arange(0,5))\n",
    "    ax.coastlines()\n",
    "    # Gridlines\n",
    "    xl = ax.gridlines(draw_labels=True);\n",
    "    xl.top_labels = False\n",
    "    xl.right_labels = False\n",
    "    # Countries\n",
    "    ax.add_feature(cartopy.feature.BORDERS, linestyle='--');\n",
    "    plt.title('MCS/day: '+str(np.round(daycount,1)))\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label('Total MCS (n)')\n",
    "   \n",
    "    \n",
    "    \n",
    "plt.tight_layout()\n",
    "#f.savefig(cnst.network_data + 'figs/LSTA/corrected_LSTA/new/wavelet_coefficients/2hOverlap/maps.png')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
